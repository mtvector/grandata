{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acffde-a523-4bc6-9fd7-d6c0487fbe02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In[1]: imports and helpers\n",
    "import os, tempfile, copy\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import xarray as xr\n",
    "import tqdm\n",
    "\n",
    "import crandata\n",
    "from crandata.chrom_io import crandata_from_bigwigs, add_bigwig_array\n",
    "\n",
    "# In[2]: make temp dirs\n",
    "tmp = tempfile.TemporaryDirectory()\n",
    "base = Path(tmp.name)\n",
    "beds = base/\"beds\";    beds.mkdir()\n",
    "bws  = base/\"bigwigs\"; bws.mkdir()\n",
    "\n",
    "# In[3]: write a tiny chrom.sizes\n",
    "chromsizes = base/\"chrom.sizes\"\n",
    "with open(chromsizes,\"w\") as f:\n",
    "    f.write(\"chr1\\t1000\\n\")\n",
    "\n",
    "# In[4]: write consensus.bed\n",
    "cons_df = pd.DataFrame({\n",
    "    \"chrom\":[\"chr1\",\"chr1\",\"chr1\"],\n",
    "    \"start\":[100,300,350],\n",
    "    \"end\":  [200,400,450],\n",
    "})\n",
    "consensus_file = base/\"consensus.bed\"\n",
    "cons_df.to_csv(consensus_file, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# reload into a named‐cols DF\n",
    "consensus = pd.read_csv(\n",
    "    consensus_file, sep=\"\\t\", header=None,\n",
    "    names=[\"chrom\",\"start\",\"end\"]\n",
    ")\n",
    "\n",
    "# In[5]: build two synthetic bigWigs\n",
    "for fname,val in [(\"one.bw\", 5.0),(\"two.bw\",4.0)]:\n",
    "    path = bws/fname\n",
    "    bw = pyBigWig.open(str(path),\"w\")\n",
    "    bw.addHeader([(\"chr1\",1000)])\n",
    "    bw.addEntries([\"chr1\"], [0], [1000], [val])\n",
    "    bw.close()\n",
    "\n",
    "# In[6]: call new constructor\n",
    "target_width = 100\n",
    "out_path = base/\"out.zarr\"\n",
    "\n",
    "adata = crandata_from_bigwigs(\n",
    "    region_table         = consensus,\n",
    "    bigwig_dir           = bws,\n",
    "    backed_path          = out_path,\n",
    "    array_name           = \"X\",        # your first track name\n",
    "    obs_dim              = \"obs\",\n",
    "    var_dim              = \"var\",\n",
    "    seq_dim              = \"bin\",\n",
    "    target_region_width  = target_width,\n",
    "    bin_stat             = \"mean\",\n",
    "    chunk_size           = 2,\n",
    "    n_bins               = 1,\n",
    "    backend              = \"zarr\",\n",
    "    tile_size            = 2,\n",
    ")\n",
    "\n",
    "print(\"Created CrAnData:\")\n",
    "print(adata)\n",
    "\n",
    "# In[7]: optionally add a second track \"Y\"\n",
    "adata = add_bigwig_array(\n",
    "    adata,\n",
    "    region_table         = consensus,\n",
    "    bigwig_dir           = bws,\n",
    "    array_name           = \"Y\",        # new track\n",
    "    obs_dim              = \"obs\",\n",
    "    var_dim              = \"var\",\n",
    "    seq_dim              = \"bin\",\n",
    "    target_region_width  = target_width,\n",
    "    bin_stat             = \"mean\",\n",
    "    chunk_size           = 2,\n",
    "    n_bins               = 1,\n",
    "    backend              = \"zarr\",\n",
    "    tile_size            = 2,\n",
    ")\n",
    "print(\"After adding Y:\")\n",
    "print(adata)\n",
    "\n",
    "# In[8]: split, sequence‐add, meta‐module demo\n",
    "crandata.train_val_test_split(adata, strategy=\"chr_auto\")\n",
    "\n",
    "# dummy genome\n",
    "fa = base/\"chr1.fa\"\n",
    "with open(fa,\"w\") as f:\n",
    "    f.write(\">chr1\\n\"+\"A\"*1000+\"\\n\")\n",
    "genome = crandata.Genome(str(fa), chrom_sizes=str(chromsizes))\n",
    "\n",
    "# add sequences\n",
    "adata = crandata.seq_io.add_genome_sequences_to_crandata(\n",
    "    adata, consensus, genome\n",
    ")\n",
    "\n",
    "# write + reload\n",
    "adata.to_zarr(str(out_path), mode=\"a\")\n",
    "adata = crandata.CrAnData.open_zarr(str(out_path))\n",
    "\n",
    "# two copies for MetaCrAnDataModule\n",
    "adata1 = copy.deepcopy(adata)\n",
    "adata2 = copy.deepcopy(adata)\n",
    "# add a var split column so DNATransform works\n",
    "adata1[\"var-_-split\"] = xr.DataArray(\n",
    "    np.full(adata1.sizes[\"var\"], \"train\"), dims=[\"var\"]\n",
    ")\n",
    "adata2[\"var-_-split\"] = xr.DataArray(\n",
    "    np.full(adata2.sizes[\"var\"], \"train\"), dims=[\"var\"]\n",
    ")\n",
    "\n",
    "# DNATransform + MetaCrAnDataModule\n",
    "transform = crandata.seq_io.DNATransform(out_len=80, random_rc=True, max_shift=5)\n",
    "meta = crandata.CrAnDataModule(\n",
    "    adatas    = [adata1,adata2],\n",
    "    batch_size= 2,\n",
    "    load_keys = {\"sequences\":\"sequences\",\"X\":\"X\"},\n",
    "    dnatransform=transform,\n",
    ")\n",
    "meta.setup(\"train\")\n",
    "\n",
    "print(\"\\nA couple of batches from CrAnDataModule:\")\n",
    "for i,batch in enumerate(tqdm.tqdm(meta.train_dataloader)):\n",
    "    print({k:v.shape for k,v in batch.items()})\n",
    "    if i>=1: break\n",
    "\n",
    "# In[9]: cleanup\n",
    "print(\"\\nTemp dir contents:\", os.listdir(base))\n",
    "tmp.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1d8904-5c44-4e8c-a7ba-8bf58b5f264d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_size': 2,\n",
       " 'genome_name': 'chr1',\n",
       " 'genome_fasta': '/scratch/fast/310700/tmpz_n93cub/chr1.fa',\n",
       " 'genome_chrom_sizes': '{\"chr1\": 1000}'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c949b3a-7ce9-4a6d-a21e-6565967f9761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>350</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chrom  start  end\n",
       "0  chr1    100  200\n",
       "1  chr1    300  400\n",
       "2  chr1    350  450"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44793f3f-907f-43aa-a33a-184dffa17b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>chrom</th>\n",
       "      <th>split</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chr1:100-200</th>\n",
       "      <td>chr1:100-200</td>\n",
       "      <td>chr1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1:300-400</th>\n",
       "      <td>chr1:300-400</td>\n",
       "      <td>chr1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr1:350-450</th>\n",
       "      <td>chr1:350-450</td>\n",
       "      <td>chr1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    region chrom  split  start  end\n",
       "chr1:100-200  chr1:100-200  chr1  train      0    0\n",
       "chr1:300-400  chr1:300-400  chr1  train      0    0\n",
       "chr1:350-450  chr1:350-450  chr1  train      0    0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.get_dataframe('var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2cb0a57-e333-4776-9786-e3eeaf1ba370",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msdfs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdfs' is not defined"
     ]
    }
   ],
   "source": [
    "sdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6455b-1f4e-4b65-8ea8-e15602c43ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should the fill in _extract_values_from_bigwig actually be 0? Can we filter var where all is 0/nan without loading everything into memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f70292f-f088-4e9d-a127-4736bb591964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import crandata\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import crested\n",
    "from tqdm import tqdm\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b454e2de-689f-432e-9621-4d88f73e3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genomes = {}\n",
    "bed_dfs = {}\n",
    "chromsizes_files = {}\n",
    "bed_files = {}\n",
    "species = ['human','macaque','mouse']\n",
    "species_codes = {'human':0,'macaque':1,'mouse':2}\n",
    "\n",
    "MAX_SHIFT = 5\n",
    "WINDOW_SIZE = 2114\n",
    "WINDOW_SIZE = WINDOW_SIZE #+ 2*MAX_SHIFT\n",
    "OFFSET = WINDOW_SIZE // 2  # e.g., 50% overlap\n",
    "N_THRESHOLD = 0.3\n",
    "n_bins = WINDOW_SIZE//50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bdeca-411d-47ea-a846-6c906784b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in species:\n",
    "    genome_path = '/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/genome/onehots/'+s\n",
    "    fasta_file = os.path.join(genome_path,s+'.fa')\n",
    "    chrom_sizes = os.path.join(genome_path,s+'.fa.sizes')\n",
    "    annotation_gtf_file = os.path.join(genome_path,s+'.annotation.gtf')\n",
    "    chromsizes_files[s] = chrom_sizes\n",
    "    genome = crandata.Genome(fasta_file, chrom_sizes, annotation_gtf_file)\n",
    "    genome.to_memory()\n",
    "    genomes[s] = genome\n",
    "    OUTPUT_BED = os.path.join(genome_path, \"binned_genome.bed\")\n",
    "    # Generate bins and optionally write to disk.\n",
    "    binned_df = crandata.bin_genome(genome, WINDOW_SIZE+ 2*MAX_SHIFT, OFFSET, n_threshold=N_THRESHOLD, output_path=OUTPUT_BED).reset_index(drop=True)\n",
    "    bed_dfs[s] = binned_df\n",
    "    print(\"Filtered bins:\")\n",
    "    print(binned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af654431-a5bc-43c2-a4b3-0d9b19adc707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-24T12:55:13.015633-0700 INFO Genome sequences loaded into memory.\n",
      "2025-04-24T12:56:38.681106-0700 INFO Genome sequences loaded into memory.\n",
      "2025-04-24T12:58:44.673891-0700 INFO Genome sequences loaded into memory.\n"
     ]
    }
   ],
   "source": [
    "bed_dfs = {}\n",
    "for s in species:\n",
    "    print(s)\n",
    "    genome_path = '/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/genome/onehots/'+s\n",
    "    fasta_file = os.path.join(genome_path,s+'.fa')\n",
    "    chrom_sizes = os.path.join(genome_path,s+'.fa.sizes')\n",
    "    annotation_gtf_file = os.path.join(genome_path,s+'.annotation.gtf')\n",
    "    chromsizes_files[s] = chrom_sizes\n",
    "    genome = crandata.Genome(fasta_file, chrom_sizes, annotation_gtf_file)\n",
    "    genome.to_memory()\n",
    "    genomes[s] = genome\n",
    "    OUTPUT_BED = os.path.join(genome_path, \"binned_genome.bed\")\n",
    "    bed_dfs[s] = pd.read_csv(OUTPUT_BED,sep='\\t',header=None)\n",
    "    bed_dfs[s].columns = ['chrom','start','end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33c314-6940-4d5e-9e1b-963e72216851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crandata.crandata = importlib.reload(crandata.crandata)\n",
    "# crandata.chrom_io = importlib.reload(crandata.chrom_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae2f48-3029-4477-9113-0685366e37eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing X:  61%|██████    | 337/556 [12:08<07:43,  2.12s/it]"
     ]
    }
   ],
   "source": [
    "adatas = {}\n",
    "\n",
    "for s in species:\n",
    "    print(s)\n",
    "    bigwig_dir = os.path.join('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/SpinalCord/manuscript/ATAC',s,'Group_bigwig')\n",
    "    adatas[s] = crandata.chrom_io.crandata_from_bigwigs(\n",
    "        bigwig_dir=bigwig_dir,\n",
    "        region_table=bed_dfs[s],\n",
    "        backed_path='/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.zarr',\n",
    "        array_name           = \"X\",        # your first track name\n",
    "        obs_dim              = \"obs\",\n",
    "        var_dim              = \"var\",\n",
    "        seq_dim              = \"seq_bins\",\n",
    "        target_region_width=WINDOW_SIZE,\n",
    "        bin_stat = 'mean',\n",
    "        tile_size=5000,\n",
    "        chunk_size=256,\n",
    "        n_bins=n_bins\n",
    "    )\n",
    "    bed = adatas[s].get_dataframe('var').loc[:,['chrom','start','end']] #good to test that this works\n",
    "    adatas[s] = crandata.seq_io.add_genome_sequences_to_crandata(adatas[s], bed, genomes[s])\n",
    "    print(adatas[s]['sequences'])\n",
    "    adatas[s]['var-_-species'] = xr.DataArray(np.repeat(species_codes[s],adatas[s].sizes['var']),dims='var').chunk({'var':adatas[s].attrs['chunk_size']})\n",
    "    adatas[s].to_zarr(adatas[s].encoding['source'],mode='a')\n",
    "    adatas[s] = crandata.crandata.CrAnData.open_zarr('/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.zarr')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fb161-8827-46ec-b600-0d20be10aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Alternate workflow to directly write icechunks, but this is ~5x slower (better to write pure zarr3 then convert the the whole store at once)\n",
    "# adatas = {}\n",
    "\n",
    "# for s in species:\n",
    "#     print(s)\n",
    "#     bigwigs_dir = os.path.join('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/SpinalCord/manuscript/ATAC',s,'Group_bigwig')\n",
    "#     adatas[s] = crandata.chrom_io.import_bigwigs(\n",
    "#         bgu=bigwigs_dir,\n",
    "#         regions_file=bed_files[s],\n",
    "#         backed_path='/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.icechunk',\n",
    "#         target_region_width=WINDOW_SIZE,\n",
    "#         chromsizes_file=chromsizes_files[s],\n",
    "#         target = 'raw',\n",
    "#         max_stochastic_shift=5,\n",
    "#         chunk_size=512,\n",
    "#         backend='icechunk',\n",
    "#         n_bins=n_bins\n",
    "#     )\n",
    "#     bed = adatas[s].get_dataframe('var').loc[:,['chrom','start','end']]\n",
    "#     adatas[s] = crandata.seq_io.add_genome_sequences_to_crandata(adatas[s], bed, genomes[s])\n",
    "#     print(adatas[s]['sequences'])\n",
    "#     adatas[s]['var-_-species'] = xr.DataArray(np.repeat(species_codes[s],adatas[s].sizes['var']),dims='var').chunk({'var':adatas[s].attrs['chunk_size']})\n",
    "#     adatas[s].to_icechunk(mode='a',commit_name='add_genome_seqs')\n",
    "#     # adatas[s].to_zarr(adatas[s].encoding['source'],mode='a')\n",
    "#     adatas[s] = crandata.crandata.CrAnData.open_icechunk('/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.icechunk')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667671b-3508-4daf-a706-92651c186854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for s in adatas.keys():\n",
    "    crandata.train_val_test_split(\n",
    "        adatas[s], strategy=\"region\", val_size=0.1, test_size=0.1, random_state=42\n",
    "    )\n",
    "    adatas[s].to_zarr(adatas[s].encoding['source'],mode='a')\n",
    "    # adatas[s].to_icechunk(mode='a',commit_name='train_val_test_split') #If you're using icechunk store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5093e8-1e69-4aed-86fa-3ca8d5396827",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adatas[s])\n",
    "print(adatas[s]['X'])\n",
    "adatas[s]['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb37aa4-ff60-47fa-921e-02b0bf665d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas = {}\n",
    "\n",
    "for s in tqdm(species):\n",
    "    # adatas[s] = crandata.CrAnData.open_icechunk('/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.icechunk',\n",
    "    #                                             cache_config={'num_bytes_chunks':int(8e9)})#Cache 8Gb\n",
    "    adatas[s] = crandata.crandata.CrAnData.open_zarr('/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a86a3db-16e3-49d1-9a88-b251a133687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crandata._module = importlib.reload(crandata._module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f5b95-91e2-4fd3-b4b5-cbc40531d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = crandata.seq_io.DNATransform(out_len=WINDOW_SIZE, random_rc=True, max_shift=MAX_SHIFT)\n",
    "\n",
    "meta_module = crandata.CrAnDataModule(\n",
    "    adatas=list(adatas.values()),\n",
    "    batch_size=48,\n",
    "    load_keys={'X': 'y','sequences':'sequence','var-_-species':'species'},\n",
    "    dnatransform=transform,\n",
    "    shuffle_dims=['obs'],\n",
    "    join='inner',\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Setup the meta module for the \"fit\" stage (train/val)\n",
    "meta_module.setup(\"train\")\n",
    "\n",
    "# Retrieve the training dataloader from the meta module and iterate over a couple of batches.\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "\n",
    "print(\"\\nIterating over a couple of training batches from CrAnDataModule:\")\n",
    "for i, batch in enumerate(tqdm(meta_train_dl)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69fe28-cef0-40c4-9d3f-07990fa1845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Run your code and write the profile data to a file.\n",
    "cProfile.run(\"\"\"\n",
    "for i, batch in tqdm(enumerate(meta_train_dl)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 10:\n",
    "        break\n",
    "\"\"\", \"profile_output.prof\")\n",
    "\n",
    "# Load the profile data from the file using pstats.\n",
    "p = pstats.Stats(\"profile_output.prof\")\n",
    "p.strip_dirs().sort_stats(\"cumtime\").print_stats(50)\n",
    "# p.strip_dirs().sort_stats('cumtime').print_stats('crandata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e25eaa-4273-4dab-8b52-a6334d0d9fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ee37c-e609-4921-9b62-9b306364b4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be94729-31cf-4da7-865c-f0de791e56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in tqdm(species):\n",
    "    adatas[s].unify_convert_chunks('/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.icechunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b3950-5fa7-4834-903c-0176cfd8d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas = {}\n",
    "for s in tqdm(species):\n",
    "    adatas[s] = crandata.CrAnData.open_icechunk('/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.icechunk',\n",
    "                                                cache_config={'num_bytes_chunks':int(8e9),'num_chunk_refs':5})#Cache 8Gb\n",
    "    # adatas[s] = crandata.crandata.CrAnData.open_zarr('/home/matthew.schmitz/Matthew/data/test_crandata/'+s+'_spc_test.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aaf99e-b31e-4f93-a15d-e6ca8aa65780",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = crandata.seq_io.DNATransform(out_len=WINDOW_SIZE, random_rc=True, max_shift=MAX_SHIFT)\n",
    "\n",
    "meta_module = crandata.CrAnDataModule(\n",
    "    adatas=list(adatas.values()),\n",
    "    batch_size=48,\n",
    "    load_keys={'X': 'y','sequences':'sequence','var-_-species':'species'},\n",
    "    dnatransform=transform,\n",
    "    shuffle_dims=['obs'],\n",
    "    join='inner',\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Setup the meta module for the \"fit\" stage (train/val)\n",
    "meta_module.setup(\"train\")\n",
    "\n",
    "# Retrieve the training dataloader from the meta module and iterate over a couple of batches.\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "\n",
    "print(\"\\nIterating over a couple of training batches from CrAnDataModule:\")\n",
    "for i, batch in enumerate(tqdm(meta_train_dl)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d8de2-6c15-4462-99a7-3395514d8bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "code = '''\n",
    "for i, batch in enumerate(tqdm(meta_train_dl)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 5:\n",
    "        break\n",
    "'''\n",
    "\n",
    "out = cProfile.run(code,sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785cffe-544d-44c1-8281-0bfed1380019",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_module.load()\n",
    "meta_train_dl = meta_module.train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc235960-3f83-4197-b679-4f12f9ffeec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "for i, batch in enumerate(tqdm(meta_train_dl)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 50:\n",
    "        break\n",
    "'''\n",
    "\n",
    "out = cProfile.run(code,sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661681c-6d1e-4441-9878-c21d0505df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = crested.tl.zoo.simple_convnet(\n",
    "    seq_len=2114, num_classes=batch['y'].shape[1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348c92d-cfce-4e39-8a7f-3df53b363b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# Create your own configuration\n",
    "# I recommend trying this for peak regression with a weighted cosine mse log loss function\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = crested.tl.losses.CosineMSELogLoss(max_weight=100, multiplier=1)\n",
    "loss = crested.tl.losses.PoissonLoss()\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(),\n",
    "    # keras.metrics.MeanSquaredError(),\n",
    "    # keras.metrics.CosineSimilarity(axis=1),\n",
    "    crested.tl.metrics.PearsonCorrelation(),\n",
    "    # crested.tl.metrics.ConcordanceCorrelationCoefficient(),\n",
    "    # crested.tl.metrics.PearsonCorrelationLog(),\n",
    "    # crested.tl.metrics.ZeroPenaltyMetric(),\n",
    "]\n",
    "\n",
    "alternative_config = crested.tl.TaskConfig(optimizer, loss, metrics)\n",
    "print(alternative_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a462b1b-2744-43a6-b53c-b45d87298283",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['sequence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5aeb3-5514-466c-b8c7-f04556666c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize some lazy model parameters *yawn*\n",
    "model_architecture(batch['sequence'].float().mean(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3716a-4a11-418e-9aad-6778019c0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = crested.tl.Crested(\n",
    "    data=meta_module,\n",
    "    model=model_architecture,\n",
    "    config=alternative_config,\n",
    "    project_name=\"mouse_biccn\",  # change to your liking\n",
    "    run_name=\"basemodel\",  # change to your liking\n",
    "    logger=None,  # or None, 'dvc', 'tensorboard'\n",
    "    seed=7,  # For reproducibility\n",
    ")\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    epochs=60,\n",
    "    learning_rate_reduce_patience=3,\n",
    "    early_stopping_patience=6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630ebb7-32a9-44ac-895d-1a81b6664737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import icechunk as ic\n",
    "store_path = '/home/matthew.schmitz/Matthew/data/test_crandata/mouse_spc_test.icechunk'\n",
    "storage_config = ic.local_filesystem_storage(store_path)\n",
    "config = ic.RepositoryConfig.default()\n",
    "config.caching = ic.CachingConfig(num_bytes_chunks=int(8e9))\n",
    "repo = ic.Repository.open(storage_config, config)\n",
    "session = repo.readonly_session(\"main\")\n",
    "ds = xr.open_zarr(session.store, consolidated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4213d-8a47-429c-8845-fbf51fc8da7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in ds.keys():\n",
    "    print(k)\n",
    "    print(ds[k].chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0285232-b6da-4934-b243-19ebc10dbbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = np.random.randint(0, ds.dims['var'] - 100)\n",
    "print(\"Random access start index:\", start_idx)\n",
    "\n",
    "t0 = time.time()\n",
    "subset1 = ds.isel(var=slice(start_idx, start_idx + 100))['X'].values\n",
    "print(\"First access time: {:.4f} sec\".format(time.time() - t0))\n",
    "\n",
    "t0 = time.time()\n",
    "subset2 = ds.isel(var=slice(start_idx, start_idx + 100))['X'].values\n",
    "print(\"Second access time: {:.4f} sec\".format(time.time() - t0))\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58c1db-1361-437c-b2af-dab51f582dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = np.random.randint(0, ds.dims['var'] - 100)\n",
    "print(\"Random access start index:\", start_idx)\n",
    "\n",
    "t0 = time.time()\n",
    "subset1 = np.array(ds.isel(var=slice(start_idx, start_idx + 100))['X'])\n",
    "print(\"First access time: {:.4f} sec\".format(time.time() - t0))\n",
    "\n",
    "t0 = time.time()\n",
    "subset2 = np.array(ds.isel(var=slice(start_idx, start_idx + 100))['X'].values)\n",
    "print(\"Second access time: {:.4f} sec\".format(time.time() - t0))\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc69d2-5846-4a91-8f6e-22a25093372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.array(ds.isel({'var':np.arange(start,start+1000)})['X'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ca12a-bf58-492e-ac0d-97541ce43dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.array(ds.isel({'var':np.arange(start,start+1000)})['X'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd5a76-9f07-4525-8c6d-4b5f3ed12aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel({'var':np.arange(start,start+1000)})['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc1fc0-dd4e-4abd-91fc-5f6744e4194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import icechunk as ic\n",
    "from icechunk.xarray import to_icechunk\n",
    "\n",
    "# Create a temporary directory for the Icechunk store\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    store_path = f\"{tmpdir}/example.icechunk\"\n",
    "    \n",
    "    # Set up local storage and repository with a caching configuration (1 MB in this demo)\n",
    "    storage_config = ic.local_filesystem_storage(store_path)\n",
    "    config = ic.RepositoryConfig.default()\n",
    "    config.caching = ic.CachingConfig(num_bytes_chunks=1024 * 1024)\n",
    "    repo = ic.Repository.create(storage_config, config)\n",
    "    \n",
    "    # Create a simple xarray dataset with a 'var' dimension and chunk it along 'var'\n",
    "    # In this example, the dataset 'X' has shape (var=1000, y=20) and chunks of size 100 along 'var'\n",
    "    data = np.random.rand(100000, 20)\n",
    "    ds = xr.Dataset({'X': (('var', 'y'), data)})\n",
    "    ds = ds.chunk({'var': 100, 'y': 20})\n",
    "    \n",
    "    # Write the dataset to the Icechunk store using a writable session\n",
    "    session = repo.writable_session(\"main\")\n",
    "    to_icechunk(ds, session)\n",
    "    commit_hash = session.commit(\"initial commit\")\n",
    "    print(\"Committed with hash:\", commit_hash)\n",
    "    \n",
    "    # Read the dataset back from the store using a read-only session\n",
    "    session = repo.readonly_session(\"main\")\n",
    "    ds2 = xr.open_zarr(session.store, consolidated=False)\n",
    "    print(\"Dataset dimensions:\", ds2.dims)\n",
    "    \n",
    "    # Test the caching behavior by timing two consecutive random accesses along 'var'\n",
    "    var_dim = ds2.dims['var']\n",
    "    # Ensure we have 100 contiguous indices available (avoid overflow)\n",
    "    start_idx = np.random.randint(0, var_dim - 100)\n",
    "    print(\"Random access start index:\", start_idx)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    subset1 = ds2.isel(var=slice(start_idx, start_idx + 100))['X'].values\n",
    "    print(\"First access time: {:.4f} sec\".format(time.time() - t0))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    subset2 = ds2.isel(var=slice(start_idx, start_idx + 100))['X'].values\n",
    "    print(\"Second access time: {:.4f} sec\".format(time.time() - t0))\n",
    "    \n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85007d-a2ed-4775-85e3-92977fb4ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccc6f2-dc6c-414c-96e3-647e991029cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse\n",
    "from pathlib import Path\n",
    "from typing import Union, Literal, List\n",
    "from crandata import CrAnData\n",
    "\n",
    "def read_h5ad_selective_to_crandata(\n",
    "    filename: Union[str, Path],\n",
    "    mode: Literal[\"r\", \"r+\"] = \"r\",\n",
    "    selected_fields: List[str] = None,\n",
    ") -> CrAnData:\n",
    "    \"\"\"\n",
    "    Read just the specified top‐level AnnData fields (e.g. \"X\",\"obs\",\"var\",\"layers\", etc.)\n",
    "    from an .h5ad file via h5py, reconstruct sparse/categorical if needed,\n",
    "    and return a CrAnData (xarray.Dataset).  This version unpacks obs/var\n",
    "    into -_- columns so we never pass a DataFrame into CrAnData.__init__.\n",
    "    \"\"\"\n",
    "    selected_fields = selected_fields or [\"X\", \"obs\", \"var\"]\n",
    "\n",
    "    # ————— Helpers (same as before) ——————————————————————————————————\n",
    "\n",
    "    def h5_tree(g):\n",
    "        out = {}\n",
    "        for k, v in g.items():\n",
    "            if isinstance(v, h5py.Group):\n",
    "                out[k] = h5_tree(v)\n",
    "            else:\n",
    "                try: out[k] = len(v)\n",
    "                except TypeError: out[k] = \"scalar\"\n",
    "        return out\n",
    "\n",
    "    def dict_to_ete3_tree(d, parent=None):\n",
    "        from ete3 import Tree\n",
    "        if parent is None: parent = Tree(name=\"root\")\n",
    "        for k, v in d.items():\n",
    "            c = parent.add_child(name=k)\n",
    "            if isinstance(v, dict):\n",
    "                dict_to_ete3_tree(v, c)\n",
    "        return parent\n",
    "\n",
    "    def ete3_tree_to_dict(t):\n",
    "        def helper(n):\n",
    "            if n.is_leaf(): return n.name\n",
    "            return {c.name: helper(c) for c in n.get_children()}\n",
    "        return {c.name: helper(c) for c in t.get_children()}\n",
    "\n",
    "    def prune_tree(tree, keep_keys):\n",
    "        t = dict_to_ete3_tree(tree)\n",
    "        keep = set()\n",
    "        for key in keep_keys:\n",
    "            for node in t.search_nodes(name=key):\n",
    "                keep.update(node.iter_ancestors())\n",
    "                keep.update(node.iter_descendants())\n",
    "                keep.add(node)\n",
    "        for n in t.traverse(\"postorder\"):\n",
    "            if n not in keep and n.up:\n",
    "                n.detach()\n",
    "        return ete3_tree_to_dict(t)\n",
    "\n",
    "    def read_h5_to_dict(group, subtree):\n",
    "        def helper(grp, sub):\n",
    "            out = {}\n",
    "            for k, v in sub.items():\n",
    "                if isinstance(v, dict):\n",
    "                    out[k] = helper(grp[k], v) if k in grp else None\n",
    "                else:\n",
    "                    if k in grp and isinstance(grp[k], h5py.Dataset):\n",
    "                        ds = grp[k]\n",
    "                        if ds.shape == ():\n",
    "                            out[k] = ds[()]\n",
    "                        else:\n",
    "                            arr = ds[...]\n",
    "                            if arr.dtype.kind == \"S\":\n",
    "                                arr = arr.astype(str)\n",
    "                            out[k] = arr\n",
    "                    else:\n",
    "                        out[k] = None\n",
    "            return out\n",
    "        return helper(group, subtree)\n",
    "\n",
    "    def convert_to_dataframe(d: dict) -> pd.DataFrame:\n",
    "        # infer length\n",
    "        length = next((len(v) for v in d.values() if not isinstance(v, dict)), None)\n",
    "        if length is None:\n",
    "            raise ValueError(\"Cannot infer obs/var length\")\n",
    "        cols = {}\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, dict) and {\"categories\",\"codes\"} <= set(v):\n",
    "                codes = np.asarray(v[\"codes\"], int)\n",
    "                cats  = [c.decode() if isinstance(c, bytes) else c for c in v[\"categories\"]]\n",
    "                if len(codes)==length:\n",
    "                    cols[k] = pd.Categorical.from_codes(codes, cats)\n",
    "            elif isinstance(v, dict) and {\"data\",\"indices\",\"indptr\"} <= set(v):\n",
    "                shape = tuple(v.get(\"shape\",(length, max(v[\"indices\"])+1)))\n",
    "                cols[k] = csr_matrix((v[\"data\"], v[\"indices\"], v[\"indptr\"]), shape=shape)\n",
    "            elif not isinstance(v, dict):\n",
    "                arr = np.asarray(v)\n",
    "                if arr.ndim==1 and arr.shape[0]==length:\n",
    "                    if arr.dtype.kind==\"S\":\n",
    "                        arr = arr.astype(str)\n",
    "                    cols[k] = arr\n",
    "        return pd.DataFrame(cols)\n",
    "\n",
    "    # ————— Read HDF5 and prune ——————————————————————————————————\n",
    "\n",
    "    with h5py.File(filename, mode) as f:\n",
    "        full_tree = h5_tree(f)\n",
    "        pruned    = prune_tree(full_tree, selected_fields)\n",
    "        raw       = read_h5_to_dict(f, pruned)\n",
    "\n",
    "    data_vars = {}\n",
    "    coords     = {}\n",
    "\n",
    "    # — obs: unpack into coords + obs-_-col ——————————————————————————————\n",
    "    if \"obs\" in raw:\n",
    "        od = raw[\"obs\"]\n",
    "        idx = od.pop(\"_index\", None)\n",
    "        obs_df = convert_to_dataframe(od)\n",
    "        if idx is not None:\n",
    "            obs_df.index = [str(x) for x in idx]\n",
    "        coords[\"obs\"] = obs_df.index.to_numpy()\n",
    "\n",
    "        # now unpack columns\n",
    "        for col in obs_df.columns:\n",
    "            data_vars[f\"obs-_-{col}\"] = xr.DataArray(\n",
    "                obs_df[col].values,\n",
    "                dims=(\"obs\",),\n",
    "                coords={\"obs\": coords[\"obs\"]}\n",
    "            )\n",
    "        # also store index\n",
    "        data_vars[\"obs-_-index\"] = xr.DataArray(coords[\"obs\"], dims=(\"obs\",))\n",
    "\n",
    "    # — var: same pattern ——————————————————————————————————————————————\n",
    "    if \"var\" in raw:\n",
    "        vd = raw[\"var\"]\n",
    "        idx = vd.pop(\"_index\", None)\n",
    "        var_df = convert_to_dataframe(vd)\n",
    "        if idx is not None:\n",
    "            var_df.index = [str(x) for x in idx]\n",
    "        coords[\"var\"] = var_df.index.to_numpy()\n",
    "\n",
    "        for col in var_df.columns:\n",
    "            data_vars[f\"var-_-{col}\"] = xr.DataArray(\n",
    "                var_df[col].values,\n",
    "                dims=(\"var\",),\n",
    "                coords={\"var\": coords[\"var\"]}\n",
    "            )\n",
    "        data_vars[\"var-_-index\"] = xr.DataArray(coords[\"var\"], dims=(\"var\",))\n",
    "\n",
    "    # — X matrix ——————————————————————————————————————————————————\n",
    "    if \"X\" in raw:\n",
    "        xraw = raw[\"X\"]\n",
    "        print(xraw)\n",
    "        if isinstance(xraw, dict) and {\"data\",\"indices\",\"indptr\"} <= set(xraw):\n",
    "            csr_mat = csr_matrix((xraw[\"data\"], xraw[\"indices\"], xraw[\"indptr\"]))\n",
    "                                  #shape=tuple(xraw[\"shape\"]))\n",
    "            arr = sparse.COO.from_scipy_sparse(csr_mat)\n",
    "        else:\n",
    "            arr = np.asarray(xraw)\n",
    "        data_vars[\"X\"] = xr.DataArray(arr, dims=(\"obs\",\"var\"), coords=coords)\n",
    "\n",
    "    # — layers/obsm/varm/obsp ——————————————————————————————————————————\n",
    "    for grp in (\"layers\",\"obsm\",\"varm\",\"obsp\"):\n",
    "        if grp in raw:\n",
    "            for name, val in raw[grp].items():\n",
    "                if val is None:\n",
    "                    continue\n",
    "                if isinstance(val, dict) and {\"data\",\"indices\",\"indptr\"} <= set(val):\n",
    "                    csr_mat = csr_matrix((val[\"data\"], val[\"indices\"], val[\"indptr\"]))\n",
    "                                          #shape=tuple(val.get(\"shape\",arr.shape)))\n",
    "                    arr = sparse.COO.from_scipy_sparse(csr_mat)\n",
    "                else:\n",
    "                    arr = np.asarray(val)\n",
    "\n",
    "                if grp==\"layers\":\n",
    "                    dims, c = (\"obs\",\"var\"), coords\n",
    "                elif grp==\"obsm\":\n",
    "                    d2 = f\"obsm_{name}\"\n",
    "                    dims, c = (\"obs\",d2), {\"obs\":coords[\"obs\"],d2:np.arange(arr.shape[1])}\n",
    "                elif grp==\"varm\":\n",
    "                    d2 = f\"varm_{name}\"\n",
    "                    dims, c = (\"var\",d2), {\"var\":coords[\"var\"],d2:np.arange(arr.shape[1])}\n",
    "                else:  # obsp\n",
    "                    d2 = f\"obsp_{name}\"\n",
    "                    dims, c = (\"obs\",d2), {\"obs\":coords[\"obs\"],d2:coords[\"obs\"]}\n",
    "\n",
    "                data_vars[f\"{grp}-_-{name}\"] = xr.DataArray(arr, dims=dims, coords=c)\n",
    "\n",
    "    # ——— Finally, build and return CrAnData ——————————————————————\n",
    "    return CrAnData(data_vars=data_vars, coords=coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b8730-2fa1-445b-8bf7-45f0023fd47d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds=read_h5ad_selective_to_crandata(\"/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/data/testgenesets/siletti300k_highly_variable.h5ad\",selected_fields=[\"X\",\"obs\",\"var\",\"UMIs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d9f35-113f-47a3-83b1-61c09734377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780ae53-9d66-4682-9837-d12524625d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from itertools import product\n",
    "from typing import Union, List, Tuple, Dict\n",
    "\n",
    "def group_aggr_xr(\n",
    "    ds: xr.Dataset,\n",
    "    array_name: str,\n",
    "    categories: Union[str, List[str]],\n",
    "    agg_func=np.mean,\n",
    "    normalize: bool = False,\n",
    ") -> Tuple[np.ndarray, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Group–aggregate an xarray.Dataset along 'obs' by one or more categorical\n",
    "    obs columns, using xarray.groupby on the specified data array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds\n",
    "        An xarray.Dataset (e.g. CrAnData) containing:\n",
    "          - a DataArray `ds[array_name]` with dims (\"obs\",\"var\") or similar,\n",
    "          - one or more obs columns named \"obs-_-<cat>\".\n",
    "    array_name\n",
    "        Name of the DataArray in `ds` to aggregate (e.g. \"X\", \"layers-_-counts\", \"obsp-_-contacts\").\n",
    "    categories\n",
    "        Single category name or list of names (the <cat> in \"obs-_-<cat>\").\n",
    "    agg_func\n",
    "        Aggregation function (e.g. np.mean, np.median, np.std).\n",
    "    normalize\n",
    "        If True, each observation is normalized by its row‑sum before grouping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : np.ndarray\n",
    "        Aggregated values, shape (*category_sizes, num_vars).\n",
    "    category_orders : dict\n",
    "        Maps each category name → list of its observed levels (in first‑appearance order).\n",
    "    \"\"\"\n",
    "    # — normalize categories list —\n",
    "    if isinstance(categories, str):\n",
    "        categories = [categories]\n",
    "    if not categories:\n",
    "        raise ValueError(\"Must supply at least one category name\")\n",
    "\n",
    "    # — pick the DataArray and its dims —\n",
    "    da = ds[array_name]\n",
    "    obs_dim, var_dim = da.dims[:2]\n",
    "    n_vars = da.sizes[var_dim]\n",
    "\n",
    "    # — collect category arrays & orders —\n",
    "    category_orders: Dict[str, List[str]] = {}\n",
    "    cat_arrs: List[np.ndarray] = []\n",
    "    for cat in categories:\n",
    "        arr = ds[f\"obs-_-{cat}\"].values.astype(str)\n",
    "        # preserve first‑appearance order\n",
    "        seen = dict.fromkeys(arr.tolist())\n",
    "        category_orders[cat] = list(seen.keys())\n",
    "        cat_arrs.append(arr)\n",
    "\n",
    "    # — build grouping coordinate —\n",
    "    if len(categories) == 1:\n",
    "        group_dim = categories[0]\n",
    "        grouping = xr.DataArray(cat_arrs[0], dims=obs_dim, coords={obs_dim: ds.coords[obs_dim]})\n",
    "    else:\n",
    "        sep = \"__\"\n",
    "        combo = cat_arrs[0]\n",
    "        for arr in cat_arrs[1:]:\n",
    "            combo = np.char.add(np.char.add(combo, sep), arr)\n",
    "        group_dim = sep.join(categories)\n",
    "        grouping = xr.DataArray(combo, dims=obs_dim, coords={obs_dim: ds.coords[obs_dim]})\n",
    "\n",
    "    da = da.assign_coords(**{group_dim: grouping})\n",
    "\n",
    "    # — optional normalize each row by its sum —\n",
    "    if normalize:\n",
    "        da = da / da.sum(dim=var_dim, keepdims=True)\n",
    "\n",
    "    # — groupby & reduce —\n",
    "    grouped = da.groupby(group_dim).reduce(agg_func, dim=obs_dim)\n",
    "\n",
    "    # — extract the raw data, densifying if needed —\n",
    "    raw = grouped.data\n",
    "    if hasattr(raw, \"todense\"):\n",
    "        arr = raw.todense()\n",
    "    elif hasattr(raw, \"toarray\"):\n",
    "        arr = raw.toarray()\n",
    "    else:\n",
    "        arr = np.asarray(raw)\n",
    "\n",
    "    # — reorder and reshape into (*category_sizes, n_vars) —\n",
    "    if len(categories) == 1:\n",
    "        cats = category_orders[categories[0]]\n",
    "        # ensure our output follows the same order\n",
    "        idx = [cats.index(v) for v in grouped[ group_dim ].values.astype(str)]\n",
    "        result = arr[idx, :]\n",
    "    else:\n",
    "        lists = [category_orders[c] for c in categories]\n",
    "        combos = list(product(*lists))\n",
    "        combo_strs = [sep.join(c) for c in combos]\n",
    "        idx = [combo_strs.index(v) for v in grouped[group_dim].values.astype(str)]\n",
    "        reshaped = arr[idx, :]\n",
    "        sizes = [len(category_orders[c]) for c in categories]\n",
    "        result = reshaped.reshape(*sizes, n_vars)\n",
    "\n",
    "    return result, category_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2828e51-33ac-44cb-abaa-9e1c370e04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_aggr_crandata_xr(ds,category_column_names=['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f31af-842d-46b1-bd2a-fca850ca9bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crested",
   "language": "python",
   "name": "crested"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
