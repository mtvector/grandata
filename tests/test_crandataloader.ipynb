{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acffde-a523-4bc6-9fd7-d6c0487fbe02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import copy\n",
    "import xarray as xr\n",
    "import tqdm\n",
    "\n",
    "# Import our new module system and utilities.\n",
    "import crandata\n",
    "from crandata import CrAnDataModule, MetaCrAnDataModule, CrAnData\n",
    "from crandata.chrom_io import import_bigwigs\n",
    "from crandata.seq_io import add_genome_sequences_to_crandata, DNATransform\n",
    "\n",
    "# Create temporary directories for synthetic data.\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "base_dir = Path(temp_dir.name)\n",
    "beds_dir = base_dir / \"beds\"\n",
    "bigwigs_dir = base_dir / \"bigwigs\"\n",
    "beds_dir.mkdir(exist_ok=True)\n",
    "bigwigs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a chromsizes file.\n",
    "chromsizes_file = base_dir / \"chrom.sizes\"\n",
    "with open(chromsizes_file, \"w\") as f:\n",
    "    f.write(\"chr1\\t1000\\n\")\n",
    "\n",
    "# Create two BED files (simulate two different classes).\n",
    "bed_data_A = pd.DataFrame({\n",
    "    0: [\"chr1\", \"chr1\"],\n",
    "    1: [100, 300],\n",
    "    2: [200, 400]\n",
    "})\n",
    "bed_data_B = pd.DataFrame({\n",
    "    0: [\"chr1\", \"chr1\"],\n",
    "    1: [150, 350],\n",
    "    2: [250, 450]\n",
    "})\n",
    "bed_file_A = beds_dir / \"ClassA.bed\"\n",
    "bed_file_B = beds_dir / \"ClassB.bed\"\n",
    "bed_data_A.to_csv(bed_file_A, sep=\"\\t\", header=False, index=False)\n",
    "bed_data_B.to_csv(bed_file_B, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# Create a consensus BED file.\n",
    "consensus = pd.DataFrame({\n",
    "    0: [\"chr1\", \"chr1\", \"chr1\"],\n",
    "    1: [100, 300, 350],\n",
    "    2: [200, 400, 450]\n",
    "})\n",
    "consensus_file = base_dir / \"consensus.bed\"\n",
    "consensus.to_csv(consensus_file, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# Create two bigWig files.\n",
    "bigwig_file1 = bigwigs_dir / \"test.bw\"\n",
    "bw1 = pyBigWig.open(str(bigwig_file1), \"w\")\n",
    "bw1.addHeader([(\"chr1\", 1000)])\n",
    "bw1.addEntries(chroms=[\"chr1\"], starts=[0], ends=[1000], values=[5.0])\n",
    "bw1.close()\n",
    "\n",
    "bigwig_file2 = bigwigs_dir / \"test2.bw\"\n",
    "bw2 = pyBigWig.open(str(bigwig_file2), \"w\")\n",
    "bw2.addHeader([(\"chr1\", 1000)])\n",
    "bw2.addEntries(chroms=[\"chr1\"], starts=[0], ends=[1000], values=[4.0])\n",
    "bw2.close()\n",
    "\n",
    "# Set extraction parameters.\n",
    "target_region_width = 100\n",
    "backed_path = base_dir / \"chrom_data.zarr\"\n",
    "\n",
    "# Create the CrAnData object from bigWig files and consensus regions.\n",
    "adata = import_bigwigs(\n",
    "    bigwigs_folder=str(bigwigs_dir),\n",
    "    regions_file=str(consensus_file),\n",
    "    backed_path=str(backed_path),\n",
    "    target_region_width=target_region_width,\n",
    "    chromsizes_file=str(chromsizes_file),\n",
    ")\n",
    "\n",
    "crandata.train_val_test_split(adata,strategy='chr_auto')\n",
    "\n",
    "# Create a dummy FASTA file for a genome.\n",
    "fasta_file = base_dir / \"chr1.fa\"\n",
    "with open(fasta_file, \"w\") as f:\n",
    "    f.write(\">chr1\\n\")\n",
    "    f.write(\"A\" * 1000 + \"\\n\")\n",
    "\n",
    "# Create a Genome object.\n",
    "from crandata._genome import Genome\n",
    "dummy_genome = Genome(str(fasta_file), chrom_sizes=str(chromsizes_file))\n",
    "\n",
    "# Add sequences to the CrAnData using the provided seq_io utility.\n",
    "# Here we use the consensus regions as our ranges.\n",
    "consensus.columns = ['chrom', 'start', 'end']\n",
    "adata = add_genome_sequences_to_crandata(adata, consensus, dummy_genome)\n",
    "\n",
    "# Write the CrAnData object to disk and then reload it to ensure sequences are out-of-memory.\n",
    "adata.to_zarr(str(backed_path),mode='a')\n",
    "adata_loaded = CrAnData.open_zarr(str(backed_path))\n",
    "print(\"Loaded CrAnData:\")\n",
    "print(adata_loaded)\n",
    "\n",
    "# Create two copies to simulate two datasets (e.g. two species), and add a \"split\" column in var metadata.\n",
    "adata1 = copy.deepcopy(adata_loaded)\n",
    "adata2 = copy.deepcopy(adata_loaded)\n",
    "adata1[\"var-_-split\"] = xr.DataArray(np.full(adata1.sizes[\"var\"], \"train\"), dims=[\"var\"])\n",
    "adata2[\"var-_-split\"] = xr.DataArray(np.full(adata2.sizes[\"var\"], \"train\"), dims=[\"var\"])\n",
    "\n",
    "# Create a DNATransform instance.\n",
    "transform = DNATransform(out_len=80, random_rc=True, max_shift=5)\n",
    "\n",
    "# Instantiate the MetaCrAnDataModule with the two datasets.\n",
    "# Note: The batch_size is now 3, matching the number of consensus regions (var dimension).\n",
    "meta_module = MetaCrAnDataModule(\n",
    "    adatas=[adata1, adata2],\n",
    "    batch_size=[2,2],        # adjust batch size to not exceed var length (3)\n",
    "    load_keys={'sequences':'sequences','X':'X'},\n",
    "    shuffle=True,\n",
    "    dnatransform=transform,\n",
    "    epoch_size=10\n",
    ")\n",
    "\n",
    "# Setup each underlying module for the \"train\" stage.\n",
    "for mod in meta_module.modules:\n",
    "    mod.setup(state=\"train\")\n",
    "\n",
    "# Retrieve the training dataloader from the meta module and iterate over a couple of batches.\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "print(\"\\nIterating over a couple of training batches from MetaCrAnDataModule:\")\n",
    "for i, batch in enumerate(tqdm.tqdm(meta_train_dl)):\n",
    "    print(batch)\n",
    "    print(f\"\\nMeta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i >= 1:\n",
    "        break\n",
    "\n",
    "print(\"\\nTemporary directory contents:\")\n",
    "print(os.listdir(base_dir))\n",
    "temp_dir.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97226999-8fcd-42c1-85cd-bb92946527ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb0a57-e333-4776-9786-e3eeaf1ba370",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6455b-1f4e-4b65-8ea8-e15602c43ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should the fill in _extract_values_from_bigwig actually be 0? Can we filter var where all is 0/nan without loading everything into memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70292f-f088-4e9d-a127-4736bb591964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import crandata\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import crested\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454e2de-689f-432e-9621-4d88f73e3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genomes = {}\n",
    "beds = {}\n",
    "chromsizes_files = {}\n",
    "bed_files = {}\n",
    "species = ['mouse','human','macaque']\n",
    "\n",
    "MAX_SHIFT = 5\n",
    "WINDOW_SIZE = 2114\n",
    "WINDOW_SIZE = WINDOW_SIZE #+ 2*MAX_SHIFT\n",
    "OFFSET = WINDOW_SIZE // 2  # e.g., 50% overlap\n",
    "N_THRESHOLD = 0.3\n",
    "n_bins = WINDOW_SIZE//50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bdeca-411d-47ea-a846-6c906784b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in species:\n",
    "    genome_path = '/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/genome/onehots/'+s\n",
    "    fasta_file = os.path.join(genome_path,s+'.fa')\n",
    "    chrom_sizes = os.path.join(genome_path,s+'.fa.sizes')\n",
    "    annotation_gtf_file = os.path.join(genome_path,s+'.annotation.gtf')\n",
    "    chromsizes_files[s] = chrom_sizes\n",
    "    genome = crandata.Genome(fasta_file, chrom_sizes, annotation_gtf_file)\n",
    "    genome.to_memory()\n",
    "    genomes[s] = genome\n",
    "    OUTPUT_BED = os.path.join(genome_path, \"binned_genome.bed\")\n",
    "    bed_files[s] = OUTPUT_BED\n",
    "    # Generate bins and optionally write to disk.\n",
    "    binned_df = crandata.bin_genome(genome, WINDOW_SIZE, OFFSET, n_threshold=N_THRESHOLD, output_path=OUTPUT_BED).reset_index(drop=True)\n",
    "    print(\"Filtered bins:\")\n",
    "    print(binned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852710e1-12db-4433-9166-a38d7bb38146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s='mouse'\n",
    "# adatas = {}\n",
    "\n",
    "# bigwigs_dir = os.path.join('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/SpinalCord/manuscript/ATAC',s,'Group_bigwig')\n",
    "# adatas[s] = crandata.chrom_io.import_bigwigs(\n",
    "#     bigwigs_folder=bigwigs_dir,\n",
    "#     regions_file='/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/genome/onehots/mouse/binned_genome_test.bed',\n",
    "#     backed_path='/home/matthew.schmitz/Matthew/'+s+'_spc_test.zarr',\n",
    "#     target_region_width=WINDOW_SIZE,\n",
    "#     chromsizes_file=chromsizes_files[s],\n",
    "#     target = 'raw',\n",
    "#     max_stochastic_shift=5,\n",
    "#     chunk_size=2048,\n",
    "#     n_bins=n_bins\n",
    "# )\n",
    "# bed = adatas[s].get_dataframe('var').loc[:,['chrom','start','end']]\n",
    "# adatas[s] = crandata.seq_io.add_genome_sequences_to_crandata(adatas[s], bed, genomes[s])\n",
    "# print(adatas[s]['sequences'])\n",
    "# adatas[s].to_zarr(adatas[s].encoding['source'],mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10492d8e-e22d-48a3-8139-6d5aadc02d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adatas = {}\n",
    "\n",
    "for s in species:\n",
    "    bigwigs_dir = os.path.join('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/SpinalCord/manuscript/ATAC',s,'Group_bigwig')\n",
    "    adatas[s] = crandata.chrom_io.import_bigwigs(\n",
    "        bigwigs_folder=bigwigs_dir,\n",
    "        regions_file=bed_files[s],\n",
    "        backed_path='/home/matthew.schmitz/Matthew/'+s+'_spc_test.zarr',\n",
    "        target_region_width=WINDOW_SIZE,\n",
    "        chromsizes_file=chromsizes_files[s],\n",
    "        target = 'raw',\n",
    "        max_stochastic_shift=5,\n",
    "        chunk_size=1024,\n",
    "        n_bins=n_bins\n",
    "    )\n",
    "    bed = adatas[s].get_dataframe('var').loc[:,['chrom','start','end']]\n",
    "    adatas[s] = crandata.seq_io.add_genome_sequences_to_crandata(adatas[s], bed, genomes[s])\n",
    "    print(adatas[s]['sequences'])\n",
    "    adatas[s].to_zarr(adatas[s].encoding['source'],mode='a')\n",
    "    # adatas[s] = crandata.crandata.CrAnData.open_zarr('/home/matthew.schmitz/Matthew/'+s+'_spc_test.zarr')\n",
    "    # # beware zarr issue with saving standard int (set to np.[u]int64)\n",
    "    # adatas[s] = adatas[s].drop_vars('sequences')\n",
    "    # adatas[s][\"var-_-start\"] = xr.DataArray(adatas[s].get_dataframe('var')['index'].str.split(\":\").str[1].str.split(\"-\").str[0].astype('int64').to_numpy(),dims=['var'])\n",
    "    # adatas[s][\"var-_-end\"] = xr.DataArray(adatas[s].get_dataframe('var')['index'].str.split(\":\").str[1].str.split(\"-\").str[1].astype('int64').to_numpy(),dims=['var'])\n",
    "    # bed = adatas[s].get_dataframe('var').loc[:,['chrom','start','end']]\n",
    "    # adatas[s] = crandata.seq_io.add_genome_sequences_to_crandata(adatas[s], bed, genomes[s])\n",
    "    # print(adatas[s]['sequences'])\n",
    "    # adatas[s].to_zarr(adatas[s].encoding['source'],mode='a')\n",
    "    adatas[s] = crandata.crandata.CrAnData.open_zarr('/home/matthew.schmitz/Matthew/'+s+'_spc_test.zarr')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca49a6-a27f-4945-b1ee-da7de9ee6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'mouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7725c-039c-4e17-8d83-40d1ffde0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas[s]['sequences'] # is it 2114 or 2124? (is the _filter_and_adjust_chromosome_data actually working?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa88d3-f313-466e-858e-9e8a20af3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# adatas['mouse'].uns['chunk_size'] = 512\n",
    "# adatas['human'].uns['chunk_size'] = 512\n",
    "# adatas['macaque'].uns['chunk_size'] = 512\n",
    "# adatas['mouse'].var[\"chunk_index\"] = np.arange(adatas['mouse'].var.shape[0]) // 512\n",
    "# adatas['human'].var[\"chunk_index\"] = np.arange(adatas['human'].var.shape[0]) // 512\n",
    "# adatas['macaque'].var[\"chunk_index\"] = np.arange(adatas['macaque'].var.shape[0]) // 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667671b-3508-4daf-a706-92651c186854",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in adatas.keys():\n",
    "    crandata.train_val_test_split(\n",
    "        adatas[s], strategy=\"region\", val_size=0.1, test_size=0.1, random_state=42\n",
    "    )\n",
    "    adatas[s].to_zarr(adatas[s].encoding['source'],mode='a')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1ca19-b8e3-412d-b96b-a711721be381",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for s in adatas.keys():\n",
    "#     adatas[s]['sequences'] = adatas[s]['sequences'].chunk({'var':2048,'seq_len':adatas[s].dims['seq_len'],'nuc':adatas[s].dims['nuc']})\n",
    "#     adatas[s]['X'] = adatas[s]['X'].chunk({'obs':adatas[s].dims['obs'],'var':2048,'seq_bins':adatas[s].dims['seq_bins']})\n",
    "#     adatas[s].to_zarr(adatas[s].encoding['source'],mode='w',safe_chunks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341acfb-d7d9-4786-8ba2-3d0ac51106df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "# crandata = importlib.reload(crandata)\n",
    "# crandata._module.MetaCrAnDataModule = importlib.reload(crandata._module.MetaCrAnDataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5093e8-1e69-4aed-86fa-3ca8d5396827",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas[s]['X'].chunksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b13d2-334c-4393-99cd-1591f90216d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas[s]['sequences'].chunksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aaf99e-b31e-4f93-a15d-e6ca8aa65780",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = crandata.seq_io.DNATransform(out_len=WINDOW_SIZE, random_rc=True, max_shift=MAX_SHIFT)\n",
    "\n",
    "meta_module = crandata.MetaCrAnDataModule(\n",
    "    adatas=list(adatas.values()),\n",
    "    batch_size=[8,8,8],\n",
    "    load_keys={'X': 'y','sequences':'sequences'},\n",
    "    dnatransform=transform,\n",
    "    num_workers=0,\n",
    "    epoch_size=1000000    # small epoch size for quick testing\n",
    ")\n",
    "\n",
    "# Setup the meta module for the \"fit\" stage (train/val)\n",
    "meta_module.setup(\"train\")\n",
    "\n",
    "# Retrieve the training dataloader from the meta module and iterate over a couple of batches.\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "\n",
    "print(\"\\nIterating over a couple of training batches from MetaAnnDataModule:\")\n",
    "for i, batch in enumerate(tqdm(meta_train_dl)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0402bad-e439-4bc3-807e-0002e340138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "code = '''\n",
    "for i, batch in enumerate(tqdm(meta_train_dl)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 3:\n",
    "        break\n",
    "'''\n",
    "\n",
    "out = cProfile.run(code,sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52795a1-dccf-4a75-a9c8-e5ee4b416051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xbatcher\n",
    "\n",
    "# Create an xarray Dataset with 5 variables of various shapes and dimensions\n",
    "ds = xr.Dataset({\n",
    "    'var1': (('time', 'lat', 'lon'), np.random.rand(20, 10, 15)),      # e.g. climate data\n",
    "    'var2': (('time', 'lat', 'lon'), np.random.rand(20, 10, 15)),                        # e.g. 2D image-like array\n",
    "    'var3': (('time', 'lat'), np.random.rand(20, 10)),\n",
    "    },\n",
    "    coords = {'time':list(range(20)),'lat':list(range(10)),'lon':list(range(15))}\n",
    ")\n",
    "\n",
    "bgen1 = xbatcher.BatchGenerator(ds=ds[['var1','var2','var3']], input_dims=dict(ds.dims))\n",
    "print(f'bgen1 has {len(bgen1)} batches')\n",
    "print(\"First batch from var1:\")\n",
    "print(bgen1[0])\n",
    "\n",
    "ds2 = xr.Dataset({\n",
    "    'var1': (('time', 'lat', 'lon'), np.random.rand(20, 8, 15)),      # e.g. climate data\n",
    "    'var2': (('time', 'lat', 'lon'), np.random.rand(20, 8, 15)),                        # e.g. 2D image-like array\n",
    "    'var3': (('time', 'lat'), np.random.rand(20, 8)), \n",
    "    },\n",
    "    coords = {'time':list(range(20)),'lat':list(range(8)),'lon':list(range(15))}\n",
    ")\n",
    "bgen2 = xbatcher.BatchGenerator(ds=ds2[['var1','var2','var3']], input_dims=dict(ds2.dims))\n",
    "print(f'bgen2 has {len(bgen2)} batches')\n",
    "print(\"First batch from var1:\")\n",
    "print(bgen2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f9fdf-0d9b-4e32-8537-72207a0e0556",
   "metadata": {},
   "outputs": [],
   "source": [
    " xr.concat([bgen1[0],bgen2[0]],dim='time',join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0c1b0-418e-4f9a-bcf4-8825ffc23651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.nodes import Mapper, MultiNodeWeightedSampler, IterableWrapper, Loader, BaseNode,ParallelMapper\n",
    "import collections\n",
    "concat_axis = 'time'\n",
    "join_param = 'inner'\n",
    "def combine_samples(x):\n",
    "    return xr.concat([next(i) for i in x],dim=concat_axis,join=join_param)\n",
    "\n",
    "datasets = IterableWrapper([bgen1, bgen2])\n",
    "\n",
    "multi_node_sampler = ParallelMapper(datasets, map_fn=combine_samples, num_workers=3, method=\"thread\")\n",
    "\n",
    "# Since nodes are iterators, they need to be manually .reset() between epochs.\n",
    "# We can wrap the root node in Loader to convert it to a more conventional Iterable.\n",
    "loader = Loader(multi_node_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b863ea-0fbc-4671-9ac2-5626718b6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import xbatcher\n",
    "from torchdata.nodes import BaseNode, IterableWrapper, Loader, ParallelMapper\n",
    "\n",
    "def combine_round_robin(*batches, concat_dim='time', join='inner'):\n",
    "    # Concatenate batches from each generator along the given dimension.\n",
    "    return xr.concat(batches, dim=concat_dim, join=join)\n",
    "\n",
    "class RoundRobinNode(BaseNode):\n",
    "    def __init__(self, nodes, combine_fn, concat_dim='time', join='inner'):\n",
    "        super().__init__()\n",
    "        self.nodes = nodes\n",
    "        self.combine_fn = combine_fn\n",
    "        self.concat_dim = concat_dim\n",
    "        self.join = join\n",
    "\n",
    "    def reset(self, initial_state=None):\n",
    "        super().reset(initial_state)\n",
    "        for node in self.nodes:\n",
    "            node.reset(initial_state)\n",
    "\n",
    "    def get_state(self):\n",
    "        return {i: node.get_state() for i, node in enumerate(self.nodes)}\n",
    "\n",
    "    def _get_next_batch(self, node):\n",
    "        # Attempt to fetch the next batch; if exhausted, reset and try again.\n",
    "        try:\n",
    "            return next(node)\n",
    "        except StopIteration:\n",
    "            node.reset()\n",
    "            return next(node)\n",
    "\n",
    "    def next(self):\n",
    "        # Use ParallelMapper to apply _get_next_batch to each node concurrently.\n",
    "        mapper = ParallelMapper(\n",
    "            source=IterableWrapper(self.nodes),\n",
    "            map_fn=self._get_next_batch,\n",
    "            num_workers=1,\n",
    "            method=\"thread\"\n",
    "        )\n",
    "        batches = list(mapper)\n",
    "        return self.combine_fn(*batches, concat_dim=self.concat_dim, join=self.join)\n",
    "\n",
    "# Example usage:\n",
    "# Assume bgen1 and bgen2 are your xbatcher BatchGenerators.\n",
    "node1 = IterableWrapper(bgen1)\n",
    "node2 = IterableWrapper(bgen2)\n",
    "\n",
    "# Create the round-robin node to concurrently retrieve a batch from each generator.\n",
    "round_robin_node = RoundRobinNode(\n",
    "    [node1, node2],\n",
    "    combine_round_robin,\n",
    "    concat_dim='time',\n",
    "    join='inner'\n",
    ")\n",
    "\n",
    "# Wrap it in a Loader for a dataloader-like interface.\n",
    "loader = Loader(round_robin_node)\n",
    "print('made loader')\n",
    "# Iterate over the loader to obtain mixed batches.\n",
    "for mixed_batch in loader:\n",
    "    print(mixed_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a0c1e-d7c8-49f9-b1df-4b413b96e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import xbatcher\n",
    "from torchdata.nodes import BaseNode, IterableWrapper, Loader\n",
    "\n",
    "# Custom node that sequentially yields batches from a list of nodes.\n",
    "class SequentialNode(BaseNode):\n",
    "    def __init__(self, nodes):\n",
    "        super().__init__()\n",
    "        self.nodes = nodes\n",
    "        self.current = 0\n",
    "\n",
    "    def reset(self, initial_state=None):\n",
    "        super().reset(initial_state)\n",
    "        for node in self.nodes:\n",
    "            node.reset(initial_state)\n",
    "        self.current = 0\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            \"current\": self.current,\n",
    "            \"states\": [node.get_state() for node in self.nodes]\n",
    "        }\n",
    "\n",
    "    def next(self):\n",
    "        # Loop until we find a node with data or we run out of nodes.\n",
    "        while self.current < len(self.nodes):\n",
    "            try:\n",
    "                # Attempt to fetch the next batch from the current node.\n",
    "                return next(self.nodes[self.current])\n",
    "            except StopIteration:\n",
    "                # If exhausted, move to the next node.\n",
    "                self.current += 1\n",
    "        # If all nodes are exhausted, signal end-of-iteration.\n",
    "        raise StopIteration\n",
    "\n",
    "# Example: Create two xbatcher BatchGenerators over different datasets.\n",
    "ds1 = xr.Dataset({\n",
    "    'var1': (('time', 'lat', 'lon'), xr.DataArray(100+np.random.rand(20, 10, 15)).data)\n",
    "})\n",
    "ds2 = xr.Dataset({\n",
    "    'var1': (('time', 'lat', 'lon'), xr.DataArray(np.random.rand(14, 10, 15)).data)\n",
    "})\n",
    "\n",
    "# Create BatchGenerators for each dataset.\n",
    "bgen1 = xbatcher.BatchGenerator(ds=ds1, input_dims={'time': 5, 'lat': 10, 'lon': 15})\n",
    "bgen2 = xbatcher.BatchGenerator(ds=ds2, input_dims={'time': 5, 'lat': 10, 'lon': 15})\n",
    "\n",
    "# Wrap each BatchGenerator in an IterableWrapper to convert them to torchdata nodes.\n",
    "node1 = IterableWrapper(bgen1)\n",
    "node2 = IterableWrapper(bgen2)\n",
    "\n",
    "# Create a SequentialNode that will iterate through node1 then node2.\n",
    "seq_node = SequentialNode([node1, node2])\n",
    "\n",
    "# Wrap the custom sequential node in a Loader for dataloader-like iteration.\n",
    "loader = Loader(seq_node)\n",
    "\n",
    "# Iterate over the loader to retrieve batches sequentially.\n",
    "for batch in loader:\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4519d5-00fb-4c78-8e10-1e12d6a45b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c24536-ed51-4016-bbeb-f334bf945d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99d0b3-99e8-4a30-8cf4-b762a1247d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(tqdm(meta_train_dl.data)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.dtype}\")\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11e2cb-b92b-4c02-bc40-886f67d2e8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "code = '''\n",
    "for i, batch in enumerate(meta_train_dl.data):\n",
    "    # print(f\"Meta Batch {i}:\")\n",
    "    # for key, tensor in batch.items():\n",
    "    #     print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 5:\n",
    "        break\n",
    "'''\n",
    "\n",
    "out = cProfile.run(code,sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661681c-6d1e-4441-9878-c21d0505df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = crested.tl.zoo.simple_convnet(\n",
    "    seq_len=2114, num_classes=batch['y'].shape[1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348c92d-cfce-4e39-8a7f-3df53b363b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# Create your own configuration\n",
    "# I recommend trying this for peak regression with a weighted cosine mse log loss function\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "loss = crested.tl.losses.CosineMSELogLoss(max_weight=100, multiplier=1)\n",
    "loss = crested.tl.losses.PoissonLoss()\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(),\n",
    "    # keras.metrics.MeanSquaredError(),\n",
    "    # keras.metrics.CosineSimilarity(axis=1),\n",
    "    crested.tl.metrics.PearsonCorrelation(),\n",
    "    # crested.tl.metrics.ConcordanceCorrelationCoefficient(),\n",
    "    # crested.tl.metrics.PearsonCorrelationLog(),\n",
    "    # crested.tl.metrics.ZeroPenaltyMetric(),\n",
    "]\n",
    "\n",
    "alternative_config = crested.tl.TaskConfig(optimizer, loss, metrics)\n",
    "print(alternative_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5aeb3-5514-466c-b8c7-f04556666c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize some lazy model parameters *yawn*\n",
    "model_architecture(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3716a-4a11-418e-9aad-6778019c0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = crested.tl.Crested(\n",
    "    data=meta_module,\n",
    "    model=model_architecture,\n",
    "    config=alternative_config,\n",
    "    project_name=\"mouse_biccn\",  # change to your liking\n",
    "    run_name=\"basemodel\",  # change to your liking\n",
    "    logger=None,  # or None, 'dvc', 'tensorboard'\n",
    "    seed=7,  # For reproducibility\n",
    ")\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    epochs=60,\n",
    "    learning_rate_reduce_patience=3,\n",
    "    early_stopping_patience=6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd5a76-9f07-4525-8c6d-4b5f3ed12aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crested",
   "language": "python",
   "name": "crested"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
